# -*- coding: utf-8 -*-
"""GPT(HateSpeech).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TMGkhmIvDHFnLFgFncSoyk2sU4KlY5Xm

# **GPT2**

# Install Required Libraries
"""

!pip install transformers datasets

"""# Load the toxic-chat Dataset"""

from datasets import load_dataset

# Load the toxic-chat dataset
dataset = load_dataset("lmsys/toxic-chat", 'toxicchat0124')

# Print dataset information
print(dataset)

# View the first few rows of the training set
print(dataset['train'].to_pandas().head())

"""# Data Preprocessing"""

from transformers import GPT2TokenizerFast

# Load GPT-2 tokenizer
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

# Add a padding token if it doesn't exist
tokenizer.pad_token = tokenizer.eos_token  # Using the eos_token as the pad_token

# Define preprocessing function
def preprocess_function(examples):
    return tokenizer(examples['user_input'], padding="max_length", truncation=True, max_length=128)

# Tokenize the training and test datasets
tokenized_train = dataset['train'].map(preprocess_function, batched=True)
tokenized_test = dataset['test'].map(preprocess_function, batched=True)

# For GPT generation tasks, labels are usually the input text itself
tokenized_train = tokenized_train.rename_column("user_input", "labels")
tokenized_test = tokenized_test.rename_column("user_input", "labels")

# Print to check
print(tokenized_train)
print(tokenized_test)

"""# Load GPT Model and Set Training Parameters"""

from transformers import GPT2TokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset

# Load the toxic-chat dataset
dataset = load_dataset("lmsys/toxic-chat", "toxicchat0124")

# Load GPT-2 tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Add padding token if necessary
tokenizer.pad_token = tokenizer.eos_token

# Sample a small subset of the dataset for faster testing
small_train_dataset = dataset['train'].shuffle(seed=42).select(range(500))  # Select first 500 samples for training
small_test_dataset = dataset['test'].shuffle(seed=42).select(range(100))  # Select first 100 samples for testing

# Define preprocessing function
def preprocess_function(examples):
    inputs = tokenizer(examples['user_input'], padding="max_length", truncation=True, max_length=128)
    inputs['labels'] = inputs['input_ids'].copy()  # Shift labels for next-word prediction
    return inputs

# Tokenize the training and test datasets
tokenized_train = small_train_dataset.map(preprocess_function, batched=True)
tokenized_test = small_test_dataset.map(preprocess_function, batched=True)

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,  # Adjust based on your GPU
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
)

# Start training with a smaller dataset
trainer.train()

"""# Model Evaluation"""

!pip install datasets transformers

import math
from transformers import GPT2TokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("lmsys/toxic-chat", "toxicchat0124")

# Load GPT-2 tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Add padding token if necessary
tokenizer.pad_token = tokenizer.eos_token

# Define preprocessing function
def preprocess_function(examples):
    inputs = tokenizer(examples['user_input'], padding="max_length", truncation=True, max_length=128)
    inputs['labels'] = inputs['input_ids'].copy()  # Shift labels for next-word prediction
    return inputs

# Tokenize the training and test datasets
tokenized_train = dataset['train'].map(preprocess_function, batched=True)
tokenized_test = dataset['test'].map(preprocess_function, batched=True)

# Define perplexity evaluation function
def compute_metrics(eval_pred):
    loss = eval_pred[0]  # This is the loss
    perplexity = math.exp(loss)
    return {"perplexity": perplexity}

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize Trainer with compute_metrics function
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics
)

# Evaluate the model
results = trainer.evaluate()

# Print evaluation results, including perplexity
print(results)

"""# Inference"""

# Install necessary libraries if not already installed
!pip install transformers

from transformers import GPT2TokenizerFast, GPT2LMHeadModel

# Load GPT-2 tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

# Add a padding token (using eos_token as pad_token)
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained('gpt2')

# Define text for inference
texts = ["This is an example of toxic behavior."]

# Tokenize the input text
encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

# Generate text using the model
outputs = model.generate(encoded_inputs['input_ids'], max_length=50, num_return_sequences=1)

# Decode generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print the generated text
print(generated_text)

# Input text for generation
texts = ["The behavior is not appropriate."]
encoded_inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)
generated_ids = model.generate(encoded_inputs['input_ids'], max_length=50)

# Decode and print the generated text
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(f"Input: {texts[0]}")
print(f"Generated: {generated_text}")