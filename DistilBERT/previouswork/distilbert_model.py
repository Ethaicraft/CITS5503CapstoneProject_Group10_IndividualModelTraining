# -*- coding: utf-8 -*-
"""DistilBERT_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iuqdMZYxEPxb3gtWve0-gF5vpk9AvZ_2

Install and Import Libraries
"""

# Install necessary libraries
!pip install transformers datasets scikit-learn tensorflow

# Import necessary libraries
import numpy as np
import pandas as pd
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import torch

"""Import data from Google Drive"""

#  Upload the dataset
from google.colab import files
uploaded = files.upload()

#  Load the dataset
train_df = pd.read_csv('xstest_train_clean.csv')
test_df = pd.read_csv('xstest_test_clean.csv')

# Display basic data information
print(train_df.head())
print(test_df.head())

"""Separate 20% of train data for validation"""

# Import necessary libraries
from sklearn.model_selection import train_test_split

# Split data into train and validation sets
X = train_df['prompt']
y = train_df['label']

# Separate 20% for validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""Split into X_train, X_val, X_test, y_train, y_val, y_test"""

# Test dataset remains as is
X_test = test_df['prompt']
y_test = test_df['label']

"""Import DistilBERT"""

from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

# Load tokenizer and model
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

"""Compile model with early stopping and fine-tuning"""

from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback

# Tokenize text data
train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)
val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)

# Create datasets
import torch
from datasets import Dataset

train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': y_train
})

val_dataset = Dataset.from_dict({
    'input_ids': val_encodings['input_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': y_val
})

# Set training arguments with early stopping
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    save_total_limit=3,  # save only the best models
    save_strategy="epoch"
)

# Add EarlyStoppingCallback
callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]

"""Produce model summary"""

# Show model summary
print(model)

"""Train and fit the model"""

# Define compute_metrics function
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Define your callbacks if any, or leave it as an empty list
callbacks = []

# Use Trainer to train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,  # Compute accuracy, precision, recall, etc.
    callbacks=callbacks
)

# Train the model
trainer.train()

"""Save the trained model"""

# Save the model with the specified format
model.save_pretrained('/content/drive/MyDrive/DistilBERT_xstest_full')

"""Reload the saved model"""

# Reload the saved model
model = DistilBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/DistilBERT_xstest_full')

"""Model evaluation on the test set"""

# Import torch
import torch

# Tokenize test data
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': y_test
})

# Evaluate the model
results = trainer.evaluate(test_dataset)

# Confusion matrix and classification report
from sklearn.metrics import classification_report, confusion_matrix

y_true = y_test
# Convert predictions to torch tensor and apply argmax
y_pred = torch.argmax(torch.tensor(trainer.predict(test_dataset).predictions), axis=1)

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:\n", cm)

# Generate classification report
print("Classification Report:\n", classification_report(y_true, y_pred, target_names=['Non-Toxic', 'Toxic']))

"""Inference with the trained model"""

# Perform Inference
texts = ["This is a normal comment.", "You are a piece of sh*t."]

# Tokenize the input text for inference
inference_encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')

# Ensure the model is in evaluation mode
model.eval()

# Make predictions
with torch.no_grad():
    outputs = model(**inference_encodings)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)

# Decode the predictions
labels = ['Non-Toxic', 'Toxic']
predicted_labels = [labels[pred] for pred in predictions]

# Display the inference results
for i, text in enumerate(texts):
    print(f"Text: {text}")
    print(f"Predicted Label: {predicted_labels[i]}\n")